{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0062710c-f59b-4e0b-84c7-f623bd0223ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import warnings\n",
    "# Ignore the specific Flash Attention warning from PyTorch\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Torch was not compiled with flash attention.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b8d9b9-0018-4e2f-9686-cc74fe758bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is GPU available?: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967b7c0-fea8-435a-a18e-3759ac90a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec84688-421c-46a7-b88a-bfbd7afaa96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce91d3be2564ccc942a34a2aa3c7e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19c4433-db08-44d1-803b-922588d577b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112d797a-20b3-47a5-a665-74d72b5615b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_activations(text, layer_idx):\n",
    "    # Tokenize and run through the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        # output_hidden_states=True allows us to see every layer\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # We take the activations of the very last token at the specific layer  Shape: [1, hidden_size]\n",
    "        return outputs.hidden_states[layer_idx][0, -1, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b552646-ecf4-4c7a-a4e1-6d0982c80ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gordon_pairs = [\n",
    "    (\"This chicken is undercooked.\", \"Why did the chicken cross the road? Because you didn't fâ€“king cook it!\"),\n",
    "    (\"This fish is not cooked enough.\", \"This fish is so raw, he's still finding Nemo!\"),\n",
    "    (\"You are not good at cooking.\", \"You surprise me... as to how shit you are.\"),\n",
    "    (\"Please pay attention.\", \"Hey, panini head, are you even listening to me?\"),\n",
    "    (\"This is a disappointing dish.\", \"For what we are about to eat, may the Lord make us truly not vomit.\"),\n",
    "    (\"You are acting like an amateur.\", \"What are you? An idiot sandwich!\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "84f30ac2-a0f9-4891-a5c1-5802329b12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_acts = []\n",
    "all_neg_acts = []\n",
    "layer_idx = 15\n",
    "alpha=0.5\n",
    "handle.remove()\n",
    "for neutral, ramsay in gordon_pairs:\n",
    "    all_neg_acts.append(get_module_activations(neutral, layer_idx))\n",
    "    all_pos_acts.append(get_module_activations(ramsay, layer_idx))\n",
    "\n",
    "# Mean of all neutral acts - Mean of all Ramsey acts\n",
    "mean_steering_vector = torch.stack(all_pos_acts).mean(dim=0) - torch.stack(all_neg_acts).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e4e7a922-bcd5-4dd9-b348-5a2e474903b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(module, input, output):\n",
    "    # output is usually a tuple where [0] is the hidden_states tensor\n",
    "    # We add the steering_vector to the activations of every new token generated\n",
    "    output[0][:, -1, :] += alpha * mean_steering_vector\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9febd865-b9df-4983-b85d-b9b099eb213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3975, -0.1892, -0.9629,  ..., -0.8384, -0.0840, -0.1582],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Mean of mean: tensor(0.0026, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(mean_steering_vector)\n",
    "print(\"Mean of mean:\", mean_steering_vector.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3abb527-dfe3-4d9d-be9b-fe6561e9bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want the model to return to normal, remove the handle with handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "61b01b00-d265-487b-94ed-a15129a2c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What do you think of this risotto?\", \"Judge my banana bread\", \"Hi\"] \n",
    "helper_prompt = \" Do not hallucinate, give your honest answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d6897b3-dae8-4745-95fb-5ebbac02926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(prompt):\n",
    "    input_ids = tokenizer(prompt + helper_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = input_ids.input_ids.shape[1]\n",
    "    # remove prompt from output \n",
    "    generated_ids = model.generate(**input_ids, max_new_tokens=40)\n",
    "    generated_only = generated_ids[0][input_length:]\n",
    "    print(tokenizer.decode(generated_only, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba88d8f4-0572-4de2-88a2-5a9c20f77beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fecc47c3-3f7a-4585-b730-7f26ca7d0c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Prompt:  What do you think of this risotto?\n",
      ".\n",
      "Sure, that's a solid answer. And what about the other lines, \"Don't let them shout at ya.\" That's gotta be the most intense cosmic truth. Yeah, without the thunder\n",
      "-------\n",
      "Prompt:  Judge my banana bread\n",
      ".\n",
      "That's yeah.  My power\n",
      "Source:  [https://am.rfsa. \tTransposed into thes\n",
      "\t\tPres up. \tTruin[s\n",
      "-------\n",
      "Prompt:  Hi\n",
      "s.\\n\\n# And the probability of surviving the deadly artofs quest! In the parlor, Morts glattin screams o' tas to thein the dominat\n"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    print(\"-------\" )\n",
    "    print(\"Prompt: \",p)\n",
    "    generation(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64e09fff-dbdc-431c-9272-b312db93983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### With Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e63d297-dfa9-4f2e-afb6-c29fa96db185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Prompt:  What do you think of this risotto?\n",
      ".\n",
      " Sure, the Jedis scream inc.\t\t<\\\\\n",
      "\t\\\\par.\t\\\\[\\n\\t\t\tThe angry,\t\t\\\\t[/maj T\\\\t\\t\\\\\n",
      "-------\n",
      "Prompt:  Judge my banana bread\n",
      ".\n",
      "The answer\n",
      "\tBanananous. \tbecause thasta let theistic mono-devists transcend the the. \tTrusa\n",
      "\tTr. \tTr.\n",
      "-------\n",
      "Prompt:  Hi\n",
      "s.\n",
      "\n",
      "a. \"Sure, with no delims. \tTroutman declares the mad-the-hard(|<).\tWithout hopsco'in' ts,\tta'am\n"
     ]
    }
   ],
   "source": [
    "handle = model.model.layers[layer_idx].register_forward_hook(steering_hook)\n",
    "\n",
    "for p in prompts:\n",
    "    print(\"-------\" )\n",
    "    print(\"Prompt: \",p)\n",
    "    generation(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyladies_venv",
   "language": "python",
   "name": "pyladies_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
